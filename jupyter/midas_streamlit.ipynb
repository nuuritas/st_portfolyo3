{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from datetime import datetime as dt\n",
    "import os \n",
    "import json\n",
    "import requests\n",
    "import concurrent.futures\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def p_no_sell(grouped):\n",
    "    grouped.insert(2,\"d_q_s\", 0)\n",
    "    grouped.insert(3,\"d_q_c\",grouped[\"d_q_b\"] - grouped[\"d_q_s\"])\n",
    "    grouped.insert(5,\"d_a_s\",0)\n",
    "    grouped[\"d_p_s\"] = 0\n",
    "\n",
    "    grouped = grouped.fillna(0)\n",
    "    grouped[\"h_q\"] = grouped[\"d_q_c\"].cumsum()\n",
    "    \n",
    "\n",
    "    grouped[\"a_a_b\"] = grouped[\"d_a_b\"].cumsum() \n",
    "    grouped[\"a_a_s\"] = grouped[\"d_a_s\"].cumsum() \n",
    "\n",
    "    grouped[\"a_p_b\"] = grouped[\"a_a_b\"] / grouped[\"h_q\"]\n",
    "\n",
    "    grouped[\"a_p_b\"] = grouped[\"a_p_b\"].apply(lambda x: round(x,2))\n",
    "    grouped[\"d_r_p\"] = 0\n",
    "    grouped[\"a_r_p\"] = 0\n",
    "    grouped.insert(9,\"h_a\",grouped[\"a_p_b\"] * grouped[\"h_q\"])\n",
    "    grouped[\"h_a\"] = grouped[\"h_a\"].apply(lambda x: round(x,2))\n",
    "    return grouped\n",
    "\n",
    "def p_buy_and_sell(grouped):\n",
    "    grouped = grouped.fillna(0)\n",
    "    grouped.insert(3,\"d_q_c\",grouped[\"d_q_b\"] - grouped[\"d_q_s\"])\n",
    "    grouped[\"h_q\"] = grouped[\"d_q_c\"].cumsum()\n",
    "    \n",
    "\n",
    "    grouped[\"a_a_b\"] = grouped[\"d_a_b\"].cumsum() \n",
    "    grouped[\"a_a_s\"] = grouped[\"d_a_s\"].cumsum() \n",
    "\n",
    "    grouped.loc[0,\"a_p_b\"] = grouped.loc[0,\"a_a_b\"] / grouped.loc[0,\"h_q\"]\n",
    "\n",
    "    for i, val in grouped.iterrows():\n",
    "        #Eğer tüm hisseler o gün satıldıysa, bir sonraki gündeki ortalama fiyat sadece yeni alınan hisselerin ortalaması olur.\n",
    "        #Eğer tüm hisseler o gün satıldıysa, ve o gün alım olmadıysa, ortalama önceki güne eşit olur.\n",
    "        if val[\"h_q\"] == 0:\n",
    "            if val[\"d_q_b\"] == 0:\n",
    "                grouped.loc[i,\"a_p_b\"] = grouped.loc[i-1,\"a_p_b\"]\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            #Eğer alış olmadıysa, eldeki maliyet değişmez.\n",
    "            if val[\"d_q_b\"] == 0:\n",
    "                grouped.loc[i,\"a_p_b\"] = grouped.loc[i-1,\"a_p_b\"]\n",
    "            else:\n",
    "                grouped.loc[i,\"a_p_b\"] = (grouped.loc[:i,\"d_a_b\"].sum() - grouped.loc[:i,\"d_a_s\"].sum()) / grouped.loc[i,\"h_q\"]\n",
    "            # grouped.loc[i,\"a_p_b\"] = val[\"a_a_b\"] / val[\"h_q\"]\n",
    "        if 0 in grouped.loc[:i,\"h_q\"].values:\n",
    "            \n",
    "            last_zero_index = grouped.loc[:i,\"h_q\"].tolist().index(0)\n",
    "            if val[\"d_q_b\"] != 0:\n",
    "                grouped.loc[i,\"a_p_b\"] = sum_product = grouped.loc[last_zero_index:i, \n",
    "                                        \"d_p_b\"].mul(grouped.loc[last_zero_index:i, \"d_q_b\"]).sum() / grouped.loc[last_zero_index:i,\"d_q_b\"].sum()\n",
    "        if val[\"d_q_s\"] > 0:\n",
    "            last_average_buy = grouped.loc[:i].query(\"d_p_b != 0\")[\"d_p_b\"].iloc[-1]\n",
    "            grouped.loc[i,\"d_r_p\"] = (val[\"d_p_s\"] - last_average_buy) * val[\"d_q_s\"]\n",
    "        else:\n",
    "            grouped.loc[i,\"d_r_p\"] = 0\n",
    "        grouped.loc[i,\"a_r_p\"] = grouped.loc[:i,\"d_r_p\"].sum()\n",
    "    \n",
    "    grouped[\"a_p_b\"] = grouped[\"a_p_b\"].apply(lambda x: round(x,2))\n",
    "        \n",
    "    grouped.insert(9,\"h_a\",grouped[\"a_p_b\"] * grouped[\"h_q\"])\n",
    "    grouped[\"h_a\"] = grouped[\"h_a\"].apply(lambda x: round(x,2))\n",
    "    return grouped\n",
    "\n",
    "def port_func1(ticker,df):\n",
    "    data = df.query(\"ticker == @ticker\")\n",
    "    data[\"date\"] = data[\"date\"].apply(lambda x: x.normalize())\n",
    "\n",
    "    df1 = data.groupby([\"date\", \"buy_sell\"]).agg({\n",
    "        \"quantity\": \"sum\",\n",
    "        \"trans_amount\": \"sum\",\n",
    "        \"price\": lambda x: (x * data.loc[x.index, \"quantity\"]).sum() / df.loc[x.index, \"quantity\"].sum()\n",
    "    }).unstack()\n",
    "\n",
    "    df1.columns = [\"_\".join(col).strip() for col in df1.columns.values]\n",
    "    df1 = df1.rename(columns={\n",
    "        \"quantity_Alış\": \"d_q_b\",\n",
    "        \"quantity_Satış\": \"d_q_s\",\n",
    "        \"trans_amount_Alış\": \"d_a_b\",\n",
    "        \"trans_amount_Satış\": \"d_a_s\",\n",
    "        \"price_Alış\": \"d_p_b\",\n",
    "        \"price_Satış\": \"d_p_s\"\n",
    "    }).reset_index()\n",
    "\n",
    "    ####Situation stock never sold:\n",
    "    if \"d_q_s\" not in df1.columns:\n",
    "        df2 = p_no_sell(df1)\n",
    "    else:\n",
    "        df2 = p_buy_and_sell(df1)\n",
    "    return ticker, df2\n",
    "\n",
    "    ticker, df3 = port_func1(ticker,df)\n",
    "\n",
    "def port_func2(ticker,df3):\n",
    "        min_date = df3[\"date\"].min()\n",
    "        if df3.loc[len(df3)-1,\"h_q\"] != 0:\n",
    "            max_date = dt.today()\n",
    "        else:\n",
    "            max_date = df3[\"date\"].max()\n",
    "        df4 = pd.DataFrame({'date': pd.date_range(start=min_date, end=max_date, freq='B').normalize()})\n",
    "        df4 = df4.merge(df3, on='date', how='left')\n",
    "        df4 = df4.fillna({\n",
    "                'd_q_b': 0,\n",
    "                'd_q_s': 0,\n",
    "                'd_a_b': 0,\n",
    "                'd_a_s': 0,\n",
    "                'd_p_b': 0,\n",
    "                'd_p_s': 0,\n",
    "                'd_q_c': 0,\n",
    "                \"d_r_p\": 0,\n",
    "            })\n",
    "        \n",
    "        df4 = df4.merge(price_data.query(\"ticker == @ticker\")[[\"date\",\"open\",\"close\"]], on=[\"date\"],how=\"left\")\n",
    "        f_fill_col = [\"h_q\", \"a_a_b\",\"a_a_s\", \"a_p_b\", \"a_r_p\",\"close\",\"open\"] \n",
    "        #min, max, vol_ö\n",
    "        df4[f_fill_col] = df4[f_fill_col].ffill()\n",
    "        \n",
    "        df4 = df4.query(\"d_q_b + d_q_s + h_q > 0\")\n",
    "        # df4 = df4.fillna(0)\n",
    "        df4[\"h_a\"] = df4[\"h_q\"] * df4[\"a_p_b\"]\n",
    "        df4['t_v'] = df4['h_q'] * df4['close']\n",
    "        \n",
    "        df4['a_ur_p'] = df4['t_v'] - df4['h_a']\n",
    "        df4['a_ur_p'] = np.where(df4['h_q'] == 0, 0, df4['a_ur_p'])\n",
    "        df4[\"d_ur_p\"] = (df4[\"close\"] - df4[\"open\"]) * df4[\"h_q\"]\n",
    "        df4[\"d_p\"] = df4[\"d_ur_p\"] + df4[\"d_r_p\"]\n",
    "        df4[\"a_p\"] = df4[\"a_ur_p\"] + df4[\"a_r_p\"]\n",
    "        df4[\"d_%\"] = (round(df4[\"close\"] / df4[\"open\"],3) - 1) * 100\n",
    "        df4[\"a_%\"] = (round(df4[\"close\"] / df4[\"a_p_b\"],3) - 1) * 100\n",
    "        df4.reset_index(drop=True, inplace=True)\n",
    "        df4[\"d_p_b\"] = df4[\"d_p_b\"].apply(lambda x: round(x,2))\n",
    "        df4[\"open\"] = df4[\"open\"].apply(lambda x: round(x,2))\n",
    "        df4.insert(1, 'ticker', ticker)\n",
    "        return df4    \n",
    "    \n",
    "def portfoy(ticker):\n",
    "    ticker, df3 = port_func1(ticker,df)\n",
    "    df4 = port_func2(ticker,df3)\n",
    "    return df4\n",
    "\n",
    "def create_gunluk_ozet(port_all):\n",
    "    selected_col = [\"date\",\"ticker\",\"h_q\",\"a_p_b\",'d_q_c',\"open\",\"close\",\"d_%\",'a_%', 'd_a_b',\"d_a_s\", 't_v',\"d_r_p\", 'a_r_p',\"d_ur_p\", 'a_ur_p']\n",
    "    gunluk_ozet_raw = port_all[selected_col]\n",
    "\n",
    "    # Group by business week\n",
    "    gunluk_ozet = gunluk_ozet_raw.groupby(pd.Grouper(key=\"date\",freq='D')).agg({\n",
    "        \"d_a_b\": 'sum',\n",
    "        \"d_a_s\":'sum',\n",
    "        \"t_v\": 'sum',\n",
    "        \"d_r_p\": 'sum',\n",
    "        \"d_ur_p\": 'sum',\n",
    "        \"a_r_p\": 'sum',\n",
    "        \"a_ur_p\": 'sum',\n",
    "    }).reset_index()\n",
    "    gunluk_ozet = gunluk_ozet[gunluk_ozet[\"date\"].isin(price_data[\"date\"].unique())]\n",
    "    gunluk_ozet[\"d_a_c\"] = - gunluk_ozet[\"d_a_b\"] + gunluk_ozet[\"d_a_s\"]\n",
    "    gunluk_ozet[\"d_a_c\"] = gunluk_ozet[\"d_a_c\"].round(2)\n",
    "    gunluk_ozet[\"t_v\"] = gunluk_ozet[\"t_v\"].round(2)\n",
    "    gunluk_ozet.insert(3,\"t_v_y\",gunluk_ozet[\"t_v\"].shift(1))\n",
    "    gunluk_ozet.loc[0,\"t_v_y\"] = gunluk_ozet.loc[0,\"d_a_b\"]\n",
    "\n",
    "    gunluk_ozet[\"d_r_p\"] = gunluk_ozet[\"d_r_p\"].round(2)\n",
    "    gunluk_ozet[\"d_ur_p\"] = gunluk_ozet[\"d_ur_p\"].round(2)\n",
    "\n",
    "    gunluk_ozet = gunluk_ozet.merge(cum_inv_df, on=\"date\", how=\"left\")\n",
    "    gunluk_ozet.rename(columns={\"cum_inv\": \"a_inv\"}, inplace=True)\n",
    "    gunluk_ozet.insert(9,\"d_inv\",gunluk_ozet[\"a_inv\"].diff())\n",
    "    gunluk_ozet.loc[0,\"d_inv\"] = gunluk_ozet.loc[0,\"a_inv\"]\n",
    "    gunluk_ozet[\"d_inv\"] = gunluk_ozet[\"d_inv\"].astype(int)\n",
    "\n",
    "    gunluk_ozet.loc[1:,\"t_v_y\"] = gunluk_ozet.loc[1:,\"t_v_y\"] + gunluk_ozet.loc[1:,\"d_inv\"]\n",
    "    gunluk_ozet.insert(4,\"d_%\",(round(gunluk_ozet[\"t_v\"] / gunluk_ozet[\"t_v_y\"],4) - 1) * 100)\n",
    "\n",
    "    gunluk_ozet[\"d_b\"] = gunluk_ozet[\"d_inv\"] + (gunluk_ozet[\"d_a_c\"])\n",
    "    gunluk_ozet[\"a_b\"] = gunluk_ozet[\"d_b\"].cumsum()\n",
    "\n",
    "    gunluk_ozet[\"a_r_p\"] = gunluk_ozet[\"a_r_p\"].round(2)\n",
    "    gunluk_ozet[\"a_ur_p\"] = gunluk_ozet[\"a_ur_p\"].round(2)\n",
    "    gunluk_ozet[\"d_p\"] = gunluk_ozet[\"d_r_p\"] + gunluk_ozet[\"d_ur_p\"]\n",
    "    gunluk_ozet[\"d_p_y\"] = round(gunluk_ozet[\"d_p\"] / gunluk_ozet[\"t_v\"],4) * 100\n",
    "\n",
    "    # gunluk_ozet.to_parquet(\"gunluk_ozet.parquet\")\n",
    "    return gunluk_ozet\n",
    "\n",
    "def create_hisse_gunluk(port_all):\n",
    "    selected_col = [\"date\",\"ticker\",\"h_q\",\"a_p_b\",'d_q_c',\"open\",\"close\",\"d_%\",'a_%', 'a_a_b', 't_v',\"d_r_p\", 'a_r_p',\"d_ur_p\", 'a_ur_p',\"d_p\",\"a_p\"]\n",
    "    hisse_gunluk = port_all[selected_col]\n",
    "    # hisse_gunluk.to_parquet(\"hisse_gunluk.parquet\")\n",
    "    return hisse_gunluk\n",
    "\n",
    "def create_haftalık_ozet(port_all):\n",
    "    selected_col = [\"date\",\"ticker\",\"h_q\",\"a_p_b\",'d_q_c',\"open\",\"close\",\"d_%\",'a_%', 'd_a_b',\"d_a_s\", 't_v',\"d_r_p\", 'a_r_p',\"d_ur_p\", 'a_ur_p',\"d_p\",\"a_p\"]\n",
    "    haftalık_data = port_all[selected_col]\n",
    "    def business_week(date):\n",
    "        # If the date is a Monday, return the date itself.\n",
    "        if date.weekday() == 0:  \n",
    "            return date\n",
    "        # Otherwise, return the date of the nearest past Monday.\n",
    "        else:\n",
    "            return date - pd.Timedelta(days=date.weekday())\n",
    "\n",
    "    # Group by business week\n",
    "    haftalık_ozet = haftalık_data.groupby([haftalık_data['date'].apply(business_week)]).agg({\n",
    "        \"d_a_b\": 'sum',\n",
    "        \"d_a_s\":'sum',\n",
    "        \"t_v\": 'sum',\n",
    "        \"d_r_p\": 'sum',\n",
    "        \"d_ur_p\": 'sum',\n",
    "        \"a_r_p\": 'sum',\n",
    "        \"a_ur_p\": 'sum',\n",
    "    }).reset_index()\n",
    "\n",
    "    haftalık_ozet = haftalık_ozet[haftalık_ozet[\"date\"].isin(price_data[\"date\"].unique())]\n",
    "    haftalık_ozet[\"d_a_c\"] = - haftalık_ozet[\"d_a_b\"] + haftalık_ozet[\"d_a_s\"]\n",
    "    haftalık_ozet[\"d_a_c\"] = haftalık_ozet[\"d_a_c\"].round(2)\n",
    "    haftalık_ozet[\"t_v\"] = haftalık_ozet[\"t_v\"].round(2)\n",
    "    haftalık_ozet.insert(3,\"t_v_y\",haftalık_ozet[\"t_v\"].shift(1))\n",
    "    haftalık_ozet.loc[0,\"t_v_y\"] = haftalık_ozet.loc[0,\"d_a_b\"]\n",
    "\n",
    "    haftalık_ozet[\"d_r_p\"] = haftalık_ozet[\"d_r_p\"].round(2)\n",
    "    haftalık_ozet[\"d_ur_p\"] = haftalık_ozet[\"d_ur_p\"].round(2)\n",
    "\n",
    "    haftalık_ozet = haftalık_ozet.merge(cum_inv_df, on=\"date\", how=\"left\")\n",
    "    haftalık_ozet.rename(columns={\"cum_inv\": \"a_inv\"}, inplace=True)\n",
    "    haftalık_ozet.insert(9,\"d_inv\",haftalık_ozet[\"a_inv\"].diff())\n",
    "    haftalık_ozet.loc[0,\"d_inv\"] = haftalık_ozet.loc[0,\"a_inv\"]\n",
    "    haftalık_ozet[\"d_inv\"] = haftalık_ozet[\"d_inv\"].astype(int)\n",
    "\n",
    "    haftalık_ozet.loc[1:,\"t_v_y\"] = haftalık_ozet.loc[1:,\"t_v_y\"] + haftalık_ozet.loc[1:,\"d_inv\"]\n",
    "    haftalık_ozet.insert(4,\"d_%\",(round(haftalık_ozet[\"t_v\"] / haftalık_ozet[\"t_v_y\"],4) - 1) * 100)\n",
    "\n",
    "    haftalık_ozet[\"d_b\"] = haftalık_ozet[\"d_inv\"] + (haftalık_ozet[\"d_a_c\"])\n",
    "    haftalık_ozet[\"a_b\"] = haftalık_ozet[\"d_b\"].cumsum()\n",
    "\n",
    "    haftalık_ozet[\"a_r_p\"] = haftalık_ozet[\"a_r_p\"].round(2)\n",
    "    haftalık_ozet[\"a_ur_p\"] = haftalık_ozet[\"a_ur_p\"].round(2)\n",
    "    haftalık_ozet[\"d_p\"] = haftalık_ozet[\"d_r_p\"] + haftalık_ozet[\"d_ur_p\"]\n",
    "    haftalık_ozet[\"d_p_y\"] = round(haftalık_ozet[\"d_p\"] / haftalık_ozet[\"t_v\"],4) * 100\n",
    "\n",
    "    # haftalık_ozet.to_parquet(\"haftalık_ozet.parquet\")\n",
    "    return haftalık_ozet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "price_data[\"date2\"] = price_data[\"date\"].dt.date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 603260 entries, 0 to 603259\n",
      "Data columns (total 7 columns):\n",
      " #   Column  Non-Null Count   Dtype                          \n",
      "---  ------  --------------   -----                          \n",
      " 0   open    603260 non-null  float64                        \n",
      " 1   high    603260 non-null  float64                        \n",
      " 2   low     603260 non-null  float64                        \n",
      " 3   close   603260 non-null  float64                        \n",
      " 4   ticker  603260 non-null  object                         \n",
      " 5   date    603260 non-null  datetime64[ns, Europe/Istanbul]\n",
      " 6   date2   603260 non-null  object                         \n",
      "dtypes: datetime64[ns, Europe/Istanbul](1), float64(4), object(2)\n",
      "memory usage: 32.2+ MB\n"
     ]
    }
   ],
   "source": [
    "price_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KAREL You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "MAVI You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "PGSUS You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "THYAO You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "MGROS You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "TUPRS You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "CIMSA You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "TAVHL You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "LOGO You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ENKAI You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "OTKAR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "KONTR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "YEOTK You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ENJSA You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "KCHOL You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "TOASO You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "SAHOL You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "GESAN You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "TTRAK You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "SMRTG You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "AHGAZ You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "OFSYM You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ASTOR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ANSGR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "IHEVA You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ALARK You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "DEVA You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "MEPET You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "DOHOL You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "UNLU You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "PRKME You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "FADE You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "DOAS You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "QUAGR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "OSMEN You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ISMEN You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "IZENR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "INFO You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ADEL You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "MPARK You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ISCTR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ISGSY You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "BYDNR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "KRDMA You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "TURSG You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "INDES You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "ADGYO You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "AKGRT You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "KLSER You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n",
      "SDTTR You are trying to merge on datetime64[ns] and datetime64[ns, Europe/Istanbul] columns for key 'date'. If you wish to proceed you should use pd.concat\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_parquet(\"../data/midas_raw/midas_df.parquet\")\n",
    "cum_inv_df = pd.read_parquet(\"../data/midas_raw/midas_cum_inv_df.parquet\")\n",
    "price_data = pd.read_parquet(\"../data/parquet/data_daily.parquet\")\n",
    "price_data[\"date\"] = price_data[\"date\"].apply(lambda x: x.normalize())\n",
    "port_all = pd.DataFrame()\n",
    "for ticker in df.ticker.unique():\n",
    "    try:\n",
    "        port_temp = portfoy(ticker)\n",
    "        port_all = pd.concat([port_all, port_temp], axis=0, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(ticker, e)\n",
    "\n",
    "port_all = port_all.query(\"ticker != 'ALTIN.S1'\")\n",
    "port_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# Define the conditions\n",
    "condition_ofsym = ((port_all['ticker'] == 'OFSYM') & (port_all['date'] >= '2023-08-16')) | (port_all['ticker'] != 'OFSYM')\n",
    "condition_adgyo = ((port_all['ticker'] == 'ADGYO') & (port_all['date'] >= '2023-09-21')) | (port_all['ticker'] != 'ADGYO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_sector_wide(data, sector_name):\n",
    "    rename_dict = {\n",
    "        \"Sektör Ortalamaları\": \"Metrics\",\n",
    "        \"F/K\": \"fk\",\n",
    "        \"PD/DD\": \"pd_dd\",\n",
    "        \"FD/FAVÖK\": \"fd_favok\"\n",
    "    }\n",
    "    \n",
    "    data = data.rename(columns=rename_dict)\n",
    "\n",
    "    \n",
    "    new_columns = {\n",
    "        \"BIST 100\": \"bist100\",\n",
    "        \"Aritmetik Ortalama\": \"ao\",\n",
    "        \"Ağırlıklı Ortalama\": \"wo\",\n",
    "        \"Medyan\": \"median\"\n",
    "    }\n",
    "\n",
    "    \n",
    "    wide_df = pd.DataFrame()\n",
    "    wide_df['sector_name'] = [sector_name]\n",
    "\n",
    "    for metric, prefix in new_columns.items():\n",
    "        for column in ['fk', 'pd_dd', 'fd_favok']:\n",
    "            col_name = f\"{prefix}_{column}\"\n",
    "            if sector_name == 'bankacilik' and column == 'fd_favok':\n",
    "                wide_df[col_name] = np.nan\n",
    "            else:\n",
    "                wide_df[col_name] = data[data['Metrics'] == metric][column].values\n",
    "\n",
    "    return wide_df\n",
    "\n",
    "# Function to convert 'Piyasa Değeri' to numerical value\n",
    "def convert_piyasa_degeri(value):\n",
    "    value = value.replace('₺', '').strip()\n",
    "    if 'mr' in value:\n",
    "        value = float(value.replace('mr', '')) * 1e3  # convert to billion\n",
    "    elif 'mn' in value:\n",
    "        value = float(value.replace('mn', ''))  # convert to million\n",
    "    return value\n",
    "\n",
    "def get_sector(sector_name):\n",
    "\n",
    "    headers = {\n",
    "        'authority': 'fintables.com',\n",
    "        'accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/avif,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.7',\n",
    "        'accept-language': 'en-US,en;q=0.9,tr;q=0.8,tr-TR;q=0.7',\n",
    "        'cache-control': 'no-cache',\n",
    "        'cookie': '_gid=GA1.2.50961081.1690710140; _gcl_au=1.1.518997462.1690710149; auth-token=eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJ0b2tlbl90eXBlIjoiYWNjZXNzIiwiZXhwIjoyMTIyNzEwMTk3LCJpYXQiOjE2OTA3MTAxOTcsImp0aSI6IjQ2NGI0YTIxYjY3ZjQ3ZDY4MmEwYjg5NWE3ZjlkMWE4IiwidXNlcl9pZCI6MTEyNzMzfQ.Bh3945i5RjYHblFOyoN_e9oqVmQcOUukFo8GqXp5wtg; _gat_UA-72451211-3=1; _ga=GA1.2.1134893438.1690710140; _ga_22JQCWWZZJ=GS1.1.1690710149.1.1.1690711335.20.0.0',\n",
    "        'dnt': '1',\n",
    "        'pragma': 'no-cache',\n",
    "        'sec-ch-ua': '\"Not.A/Brand\";v=\"8\", \"Chromium\";v=\"114\", \"Google Chrome\";v=\"114\"',\n",
    "        'sec-ch-ua-mobile': '?0',\n",
    "        'sec-ch-ua-platform': '\"macOS\"',\n",
    "        'sec-fetch-dest': 'document',\n",
    "        'sec-fetch-mode': 'navigate',\n",
    "        'sec-fetch-site': 'same-origin',\n",
    "        'sec-fetch-user': '?1',\n",
    "        'upgrade-insecure-requests': '1',\n",
    "        'user-agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/114.0.0.0 Safari/537.36'\n",
    "    }\n",
    "\n",
    "    response = requests.get(f'https://fintables.com/sektorler/{sector_name}', headers=headers)\n",
    "\n",
    "    # The content of the response\n",
    "    from bs4 import BeautifulSoup\n",
    "    soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "    sektor_ozet = soup.find_all('table', class_=\"min-w-full\")[0]\n",
    "    sektor_ozet2 = str(sektor_ozet).replace(\".\",\"\").replace(',', '.')\n",
    "    sektor_ozet_df = pd.read_html(str(sektor_ozet2))[0]\n",
    "    sektor_ozet_wide = convert_sector_wide(sektor_ozet_df, sector_name)\n",
    "    \n",
    "    my_table = soup.find_all('table', class_=\"min-w-full\")[1]\n",
    "    my_table2 = str(my_table).replace(\".\",\"\").replace(',', '.')\n",
    "    df = pd.read_html(str(my_table2))[0]\n",
    "    \n",
    "    df['Piyasa Değeri'] = df['Piyasa Değeri'].apply(convert_piyasa_degeri)\n",
    "    #df['Piyasa Değeri'] = df['Piyasa Değeri'].astype(int)\n",
    "    df[\"sector\"] = sector_name\n",
    "\n",
    "    return sektor_ozet_wide, df\n",
    "\n",
    "def get_sector_multiple(sector_names):\n",
    "    ozet_list = []\n",
    "    sirket_list = []\n",
    "\n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "        for sektor_ozet,tum_sirketler in tqdm(executor.map(get_sector, sector_names), total=len(sector_names), desc=\"Fintables Şirketler\"):\n",
    "            try:\n",
    "                sirket_list.append(tum_sirketler)\n",
    "                ozet_list.append(sektor_ozet)\n",
    "            except Exception as e:\n",
    "                print(\"Error: \", e)\n",
    "    sirket_df = pd.concat(sirket_list, axis=0, ignore_index=True)\n",
    "    ozet_df = pd.concat(ozet_list, axis=0, ignore_index=True)\n",
    "\n",
    "    sirket_df['Şirket Kodu'] = sirket_df['Şirket Kodu'].str[:-7]\n",
    "    # sirket_df['Piyasa Değeri'] = sirket_df['Piyasa Değeri'].astype(float)\n",
    "\n",
    "    sirket_df.columns = ['sirket_kodu', 'piyasa_degeri', 'fk', 'pd_dd', 'fd_favok', 'sector']\n",
    "    return ozet_df, sirket_df\n",
    "\n",
    "sector_names = json.load(open('sector_names.json',encoding=\"utf-8\"))\n",
    "\n",
    "print(\"Fintables Sektörler ve Şirketler Güncelleniyor\")\n",
    "ozet_df, sirket_df = get_sector_multiple(sector_names)\n",
    "\n",
    "all_tickers = sirket_df['sirket_kodu'].unique()\n",
    "all_tickers = list(all_tickers[:10])\n",
    "all_tickers.append('XU100')\n",
    "data_list = []\n",
    "\n",
    "def fetch_data(ticker):\n",
    "    data = tv.get_hist(symbol=ticker, exchange='BIST', interval=Interval.in_daily, n_bars=200)\n",
    "    return data\n",
    "\n",
    "# Use a ThreadPoolExecutor to fetch data in parallel\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    # Wrap the executor and the ticker list with tqdm for a progress bar\n",
    "    data_list = list(tqdm(executor.map(fetch_data, all_tickers), total=len(all_tickers)))\n",
    "\n",
    "\n",
    "data = pd.concat(data_list).reset_index()\n",
    "data[\"symbol\"] = data[\"symbol\"].str[5:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def p_no_sell(grouped):\n",
    "    grouped.insert(2,\"d_q_s\", 0)\n",
    "    grouped.insert(3,\"d_q_c\",grouped[\"d_q_b\"] - grouped[\"d_q_s\"])\n",
    "    grouped.insert(5,\"d_a_s\",0)\n",
    "    grouped[\"d_p_s\"] = 0\n",
    "\n",
    "    grouped = grouped.fillna(0)\n",
    "    grouped[\"h_q\"] = grouped[\"d_q_c\"].cumsum()\n",
    "    \n",
    "\n",
    "    grouped[\"a_a_b\"] = grouped[\"d_a_b\"].cumsum() \n",
    "    grouped[\"a_a_s\"] = grouped[\"d_a_s\"].cumsum() \n",
    "\n",
    "    grouped[\"a_p_b\"] = grouped[\"a_a_b\"] / grouped[\"h_q\"]\n",
    "\n",
    "    grouped[\"a_p_b\"] = grouped[\"a_p_b\"].apply(lambda x: round(x,2))\n",
    "    grouped[\"d_r_p\"] = 0\n",
    "    grouped[\"a_r_p\"] = 0\n",
    "    grouped.insert(9,\"h_a\",grouped[\"a_p_b\"] * grouped[\"h_q\"])\n",
    "    grouped[\"h_a\"] = grouped[\"h_a\"].apply(lambda x: round(x,2))\n",
    "    return grouped\n",
    "\n",
    "def p_buy_and_sell(grouped):\n",
    "    grouped = grouped.fillna(0)\n",
    "    grouped.insert(3,\"d_q_c\",grouped[\"d_q_b\"] - grouped[\"d_q_s\"])\n",
    "    grouped[\"h_q\"] = grouped[\"d_q_c\"].cumsum()\n",
    "    \n",
    "\n",
    "    grouped[\"a_a_b\"] = grouped[\"d_a_b\"].cumsum() \n",
    "    grouped[\"a_a_s\"] = grouped[\"d_a_s\"].cumsum() \n",
    "\n",
    "    grouped.loc[0,\"a_p_b\"] = grouped.loc[0,\"a_a_b\"] / grouped.loc[0,\"h_q\"]\n",
    "\n",
    "    for i, val in grouped.iterrows():\n",
    "        #Eğer tüm hisseler o gün satıldıysa, bir sonraki gündeki ortalama fiyat sadece yeni alınan hisselerin ortalaması olur.\n",
    "        #Eğer tüm hisseler o gün satıldıysa, ve o gün alım olmadıysa, ortalama önceki güne eşit olur.\n",
    "        if val[\"h_q\"] == 0:\n",
    "            if val[\"d_q_b\"] == 0:\n",
    "                grouped.loc[i,\"a_p_b\"] = grouped.loc[i-1,\"a_p_b\"]\n",
    "            else:\n",
    "                pass\n",
    "        else:\n",
    "            #Eğer alış olmadıysa, eldeki maliyet değişmez.\n",
    "            if val[\"d_q_b\"] == 0:\n",
    "                grouped.loc[i,\"a_p_b\"] = grouped.loc[i-1,\"a_p_b\"]\n",
    "            else:\n",
    "                grouped.loc[i,\"a_p_b\"] = (grouped.loc[:i,\"d_a_b\"].sum() - grouped.loc[:i,\"d_a_s\"].sum()) / grouped.loc[i,\"h_q\"]\n",
    "            # grouped.loc[i,\"a_p_b\"] = val[\"a_a_b\"] / val[\"h_q\"]\n",
    "        if 0 in grouped.loc[:i,\"h_q\"].values:\n",
    "            \n",
    "            last_zero_index = grouped.loc[:i,\"h_q\"].tolist().index(0)\n",
    "            if val[\"d_q_b\"] != 0:\n",
    "                grouped.loc[i,\"a_p_b\"] = sum_product = grouped.loc[last_zero_index:i, \n",
    "                                        \"d_p_b\"].mul(grouped.loc[last_zero_index:i, \"d_q_b\"]).sum() / grouped.loc[last_zero_index:i,\"d_q_b\"].sum()\n",
    "        if val[\"d_q_s\"] > 0:\n",
    "            last_average_buy = grouped.loc[:i].query(\"d_p_b != 0\")[\"d_p_b\"].iloc[-1]\n",
    "            grouped.loc[i,\"d_r_p\"] = (val[\"d_p_s\"] - last_average_buy) * val[\"d_q_s\"]\n",
    "        else:\n",
    "            grouped.loc[i,\"d_r_p\"] = 0\n",
    "        grouped.loc[i,\"a_r_p\"] = grouped.loc[:i,\"d_r_p\"].sum()\n",
    "    \n",
    "    grouped[\"a_p_b\"] = grouped[\"a_p_b\"].apply(lambda x: round(x,2))\n",
    "        \n",
    "    grouped.insert(9,\"h_a\",grouped[\"a_p_b\"] * grouped[\"h_q\"])\n",
    "    grouped[\"h_a\"] = grouped[\"h_a\"].apply(lambda x: round(x,2))\n",
    "    return grouped\n",
    "\n",
    "def port_func1(ticker,df):\n",
    "    data = df.query(\"ticker == @ticker\")\n",
    "    data[\"date\"] = data[\"date\"].apply(lambda x: x.normalize())\n",
    "\n",
    "    df1 = data.groupby([\"date\", \"buy_sell\"]).agg({\n",
    "        \"quantity\": \"sum\",\n",
    "        \"trans_amount\": \"sum\",\n",
    "        \"price\": lambda x: (x * data.loc[x.index, \"quantity\"]).sum() / df.loc[x.index, \"quantity\"].sum()\n",
    "    }).unstack()\n",
    "\n",
    "    df1.columns = [\"_\".join(col).strip() for col in df1.columns.values]\n",
    "    df1 = df1.rename(columns={\n",
    "        \"quantity_Alış\": \"d_q_b\",\n",
    "        \"quantity_Satış\": \"d_q_s\",\n",
    "        \"trans_amount_Alış\": \"d_a_b\",\n",
    "        \"trans_amount_Satış\": \"d_a_s\",\n",
    "        \"price_Alış\": \"d_p_b\",\n",
    "        \"price_Satış\": \"d_p_s\"\n",
    "    }).reset_index()\n",
    "\n",
    "    ####Situation stock never sold:\n",
    "    if \"d_q_s\" not in df1.columns:\n",
    "        df2 = p_no_sell(df1)\n",
    "    else:\n",
    "        df2 = p_buy_and_sell(df1)\n",
    "    return ticker, df2\n",
    "\n",
    "    ticker, df3 = port_func1(ticker,df)\n",
    "\n",
    "def port_func2(ticker,df3):\n",
    "        min_date = df3[\"date\"].min()\n",
    "        if df3.loc[len(df3)-1,\"h_q\"] != 0:\n",
    "            max_date = dt.today()\n",
    "        else:\n",
    "            max_date = df3[\"date\"].max()\n",
    "        df4 = pd.DataFrame({'date': pd.date_range(start=min_date, end=max_date, freq='B').normalize()})\n",
    "        df4 = df4.merge(df3, on='date', how='left')\n",
    "        df4 = df4.fillna({\n",
    "                'd_q_b': 0,\n",
    "                'd_q_s': 0,\n",
    "                'd_a_b': 0,\n",
    "                'd_a_s': 0,\n",
    "                'd_p_b': 0,\n",
    "                'd_p_s': 0,\n",
    "                'd_q_c': 0,\n",
    "                \"d_r_p\": 0,\n",
    "            })\n",
    "        \n",
    "        df4 = df4.merge(tvdata.query(\"ticker == @ticker\")[[\"date\",\"open\",\"close\"]], on=[\"date\"],how=\"left\")\n",
    "        f_fill_col = [\"h_q\", \"a_a_b\",\"a_a_s\", \"a_p_b\", \"a_r_p\",\"close\",\"open\"] \n",
    "        #min, max, vol_ö\n",
    "        df4[f_fill_col] = df4[f_fill_col].ffill()\n",
    "        \n",
    "        df4 = df4.query(\"d_q_b + d_q_s + h_q > 0\")\n",
    "        # df4 = df4.fillna(0)\n",
    "        df4[\"h_a\"] = df4[\"h_q\"] * df4[\"a_p_b\"]\n",
    "        df4['t_v'] = df4['h_q'] * df4['close']\n",
    "        \n",
    "        df4['a_ur_p'] = df4['t_v'] - df4['h_a']\n",
    "        df4['a_ur_p'] = np.where(df4['h_q'] == 0, 0, df4['a_ur_p'])\n",
    "        df4[\"d_ur_p\"] = (df4[\"close\"] - df4[\"open\"]) * df4[\"h_q\"]\n",
    "        df4[\"d_p\"] = df4[\"d_ur_p\"] + df4[\"d_r_p\"]\n",
    "        df4[\"a_p\"] = df4[\"a_ur_p\"] + df4[\"a_r_p\"]\n",
    "        df4[\"d_%\"] = (round(df4[\"close\"] / df4[\"open\"],3) - 1) * 100\n",
    "        df4[\"a_%\"] = (round(df4[\"close\"] / df4[\"a_p_b\"],3) - 1) * 100\n",
    "        df4.reset_index(drop=True, inplace=True)\n",
    "        df4[\"d_p_b\"] = df4[\"d_p_b\"].apply(lambda x: round(x,2))\n",
    "        df4[\"open\"] = df4[\"open\"].apply(lambda x: round(x,2))\n",
    "        df4.insert(1, 'ticker', ticker)\n",
    "        return df4    \n",
    "    \n",
    "def portfoy(ticker):\n",
    "    ticker, df3 = port_func1(ticker,df)\n",
    "    df4 = port_func2(ticker,df3)\n",
    "    return df4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_parquet(\"../data/midas_raw/midas_df.parquet\")\n",
    "cum_inv_df = pd.read_parquet(\"../data/midas_raw/midas_cum_inv_df.parquet\")\n",
    "tvdata = pd.read_parquet(\"../data/parquet/tvdata23.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "port_all = pd.DataFrame()\n",
    "for ticker in df.ticker.unique():\n",
    "    try:\n",
    "        port_temp = portfoy(ticker)\n",
    "        port_all = pd.concat([port_all, port_temp], axis=0, ignore_index=True)\n",
    "    except Exception as e:\n",
    "        print(ticker, e)\n",
    "port_all = port_all.query(\"ticker != 'ALTIN.S1'\")\n",
    "port_all.reset_index(drop=True, inplace=True)\n",
    "# port_all.to_parquet(\"../streamlit/portfolyo/port_all.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditions\n",
    "condition_ofsym = ((port_all['ticker'] == 'OFSYM') & (port_all['date'] >= '2023-08-16')) | (port_all['ticker'] != 'OFSYM')\n",
    "condition_adgyo = ((port_all['ticker'] == 'ADGYO') & (port_all['date'] >= '2023-09-21')) | (port_all['ticker'] != 'ADGYO')\n",
    "\n",
    "# Combine the conditions and filter the DataFrame\n",
    "port_all = port_all[condition_ofsym & condition_adgyo]\n",
    "\n",
    "selected_col = [\"date\",\"ticker\",\"h_q\",\"a_p_b\",'d_q_c',\"open\",\"close\",\"d_%\",'a_%', 'a_a_b', 't_v',\"d_r_p\", 'a_r_p',\"d_ur_p\", 'a_ur_p',\"d_p\",\"a_p\"]\n",
    "hisse_gunluk = port_all[selected_col]\n",
    "\n",
    "selected_col2 = [\"date\",\"ticker\",\"h_q\",\"a_p_b\",'d_q_c',\"open\",\"close\",\"d_%\",'a_%', 'd_a_b',\"d_a_s\", 't_v',\"d_r_p\", 'a_r_p',\"d_ur_p\", 'a_ur_p']\n",
    "gunluk_ozet_raw = port_all[selected_col2]\n",
    "\n",
    "# Group by business week\n",
    "gunluk_ozet = gunluk_ozet_raw.groupby(pd.Grouper(key=\"date\",freq='D')).agg({\n",
    "    \"d_a_b\": 'sum',\n",
    "    \"d_a_s\":'sum',\n",
    "    \"t_v\": 'sum',\n",
    "    \"d_r_p\": 'sum',\n",
    "    \"d_ur_p\": 'sum',\n",
    "    \"a_r_p\": 'sum',\n",
    "    \"a_ur_p\": 'sum',\n",
    "}).reset_index()\n",
    "gunluk_ozet = gunluk_ozet[gunluk_ozet[\"date\"].isin(tvdata[\"date\"].unique())]\n",
    "gunluk_ozet[\"d_a_c\"] = - gunluk_ozet[\"d_a_b\"] + gunluk_ozet[\"d_a_s\"]\n",
    "gunluk_ozet[\"d_a_c\"] = gunluk_ozet[\"d_a_c\"].round(2)\n",
    "gunluk_ozet[\"t_v\"] = gunluk_ozet[\"t_v\"].round(2)\n",
    "gunluk_ozet.insert(3,\"t_v_y\",gunluk_ozet[\"t_v\"].shift(1))\n",
    "gunluk_ozet.loc[0,\"t_v_y\"] = gunluk_ozet.loc[0,\"d_a_b\"]\n",
    "\n",
    "gunluk_ozet[\"d_r_p\"] = gunluk_ozet[\"d_r_p\"].round(2)\n",
    "gunluk_ozet[\"d_ur_p\"] = gunluk_ozet[\"d_ur_p\"].round(2)\n",
    "\n",
    "gunluk_ozet = gunluk_ozet.merge(cum_inv_df, on=\"date\", how=\"left\")\n",
    "gunluk_ozet.rename(columns={\"cum_inv\": \"a_inv\"}, inplace=True)\n",
    "gunluk_ozet.insert(9,\"d_inv\",gunluk_ozet[\"a_inv\"].diff())\n",
    "gunluk_ozet.loc[0,\"d_inv\"] = gunluk_ozet.loc[0,\"a_inv\"]\n",
    "gunluk_ozet[\"d_inv\"] = gunluk_ozet[\"d_inv\"].astype(int)\n",
    "\n",
    "gunluk_ozet.loc[1:,\"t_v_y\"] = gunluk_ozet.loc[1:,\"t_v_y\"] + gunluk_ozet.loc[1:,\"d_inv\"]\n",
    "gunluk_ozet.insert(4,\"d_%\",(round(gunluk_ozet[\"t_v\"] / gunluk_ozet[\"t_v_y\"],4) - 1) * 100)\n",
    "\n",
    "gunluk_ozet[\"d_b\"] = gunluk_ozet[\"d_inv\"] + (gunluk_ozet[\"d_a_c\"])\n",
    "gunluk_ozet[\"a_b\"] = gunluk_ozet[\"d_b\"].cumsum()\n",
    "\n",
    "gunluk_ozet[\"a_r_p\"] = gunluk_ozet[\"a_r_p\"].round(2)\n",
    "gunluk_ozet[\"a_ur_p\"] = gunluk_ozet[\"a_ur_p\"].round(2)\n",
    "gunluk_ozet[\"d_p\"] = gunluk_ozet[\"d_r_p\"] + gunluk_ozet[\"d_ur_p\"]\n",
    "gunluk_ozet[\"d_p_y\"] = round(gunluk_ozet[\"d_p\"] / gunluk_ozet[\"t_v\"],4) * 100\n",
    "\n",
    "# # gunluk_ozet.to_parquet(\"gunluk_ozet.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_col = [\"date\",\"ticker\",\"h_q\",\"a_p_b\",'d_q_c',\"open\",\"close\",\"d_%\",'a_%', 'd_a_b',\"d_a_s\", 't_v',\"d_r_p\", 'a_r_p',\"d_ur_p\", 'a_ur_p',\"d_p\",\"a_p\"]\n",
    "haftalık_data = port_all[selected_col]\n",
    "def business_week(date):\n",
    "    # If the date is a Monday, return the date itself.\n",
    "    if date.weekday() == 0:  \n",
    "        return date\n",
    "    # Otherwise, return the date of the nearest past Monday.\n",
    "    else:\n",
    "        return date - pd.Timedelta(days=date.weekday())\n",
    "\n",
    "# Group by business week\n",
    "haftalık_ozet = haftalık_data.groupby([haftalık_data['date'].apply(business_week)]).agg({\n",
    "    \"d_a_b\": 'sum',\n",
    "    \"d_a_s\":'sum',\n",
    "    \"t_v\": 'sum',\n",
    "    \"d_r_p\": 'sum',\n",
    "    \"d_ur_p\": 'sum',\n",
    "    \"a_r_p\": 'sum',\n",
    "    \"a_ur_p\": 'sum',\n",
    "}).reset_index()\n",
    "\n",
    "haftalık_ozet = haftalık_ozet[haftalık_ozet[\"date\"].isin(tvdata[\"date\"].unique())]\n",
    "haftalık_ozet[\"d_a_c\"] = - haftalık_ozet[\"d_a_b\"] + haftalık_ozet[\"d_a_s\"]\n",
    "haftalık_ozet[\"d_a_c\"] = haftalık_ozet[\"d_a_c\"].round(2)\n",
    "haftalık_ozet[\"t_v\"] = haftalık_ozet[\"t_v\"].round(2)\n",
    "haftalık_ozet.insert(3,\"t_v_y\",haftalık_ozet[\"t_v\"].shift(1))\n",
    "haftalık_ozet.loc[0,\"t_v_y\"] = haftalık_ozet.loc[0,\"d_a_b\"]\n",
    "\n",
    "haftalık_ozet[\"d_r_p\"] = haftalık_ozet[\"d_r_p\"].round(2)\n",
    "haftalık_ozet[\"d_ur_p\"] = haftalık_ozet[\"d_ur_p\"].round(2)\n",
    "\n",
    "haftalık_ozet = haftalık_ozet.merge(cum_inv_df, on=\"date\", how=\"left\")\n",
    "haftalık_ozet.rename(columns={\"cum_inv\": \"a_inv\"}, inplace=True)\n",
    "haftalık_ozet.insert(9,\"d_inv\",haftalık_ozet[\"a_inv\"].diff())\n",
    "haftalık_ozet.loc[0,\"d_inv\"] = haftalık_ozet.loc[0,\"a_inv\"]\n",
    "haftalık_ozet[\"d_inv\"] = haftalık_ozet[\"d_inv\"].astype(int)\n",
    "\n",
    "haftalık_ozet.loc[1:,\"t_v_y\"] = haftalık_ozet.loc[1:,\"t_v_y\"] + haftalık_ozet.loc[1:,\"d_inv\"]\n",
    "haftalık_ozet.insert(4,\"d_%\",(round(haftalık_ozet[\"t_v\"] / haftalık_ozet[\"t_v_y\"],4) - 1) * 100)\n",
    "\n",
    "haftalık_ozet[\"d_b\"] = haftalık_ozet[\"d_inv\"] + (haftalık_ozet[\"d_a_c\"])\n",
    "haftalık_ozet[\"a_b\"] = haftalık_ozet[\"d_b\"].cumsum()\n",
    "\n",
    "haftalık_ozet[\"a_r_p\"] = haftalık_ozet[\"a_r_p\"].round(2)\n",
    "haftalık_ozet[\"a_ur_p\"] = haftalık_ozet[\"a_ur_p\"].round(2)\n",
    "haftalık_ozet[\"d_p\"] = haftalık_ozet[\"d_r_p\"] + haftalık_ozet[\"d_ur_p\"]\n",
    "haftalık_ozet[\"d_p_y\"] = round(haftalık_ozet[\"d_p\"] / haftalık_ozet[\"t_v\"],4) * 100\n",
    "\n",
    "# haftalık_ozet\n",
    "# haftalık_ozet.to_parquet(\"haftalık_ozet.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'04-10-2023'"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datetime import datetime, timedelta\n",
    "now = datetime.now()\n",
    "if now.weekday() >= 5:  # 5: Saturday, 6: Sunday\n",
    "    days_to_subtract = now.weekday() - 4\n",
    "    today = now.date() - timedelta(days=days_to_subtract)\n",
    "    today_str = (now - timedelta(days=days_to_subtract)).strftime(\"%d-%m-%Y\")\n",
    "else:\n",
    "    if now.hour < 18:\n",
    "        today = now.date() - timedelta(days=1)\n",
    "        today_str = (now - timedelta(days=1)).strftime(\"%d-%m-%Y\")\n",
    "    else:\n",
    "        today = now.date()\n",
    "        today_str = now.strftime(\"%d-%m-%Y\")\n",
    "today_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "toplam_buyukluk = port_all.query(\"date == @today\").t_v.sum()\n",
    "gunluk_net = gunluk_ozet.query(\"date == @today\").d_p.values[0]\n",
    "gunluk_yuzde = gunluk_ozet.query(\"date == @today\").d_p_y.values[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'name': 'ADGYO', 'value': 1039.7},\n",
       " {'name': 'SDTTR', 'value': 5070.8},\n",
       " {'name': 'KLSER', 'value': 5236.0},\n",
       " {'name': 'ISGSY', 'value': 5481.0},\n",
       " {'name': 'TURSG', 'value': 6358.1},\n",
       " {'name': 'MGROS', 'value': 7442.3},\n",
       " {'name': 'ANSGR', 'value': 9036.4},\n",
       " {'name': 'MPARK', 'value': 11475.0},\n",
       " {'name': 'OSMEN', 'value': 11671.8},\n",
       " {'name': 'KAREL', 'value': 14805.0},\n",
       " {'name': 'ISCTR', 'value': 19461.4}]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "son_hafta = gunluk_ozet[-7:]\n",
    "son_hafta.reset_index(drop=True, inplace=True)\n",
    "haftalik_net = (\n",
    "    son_hafta.iloc[-1][\"t_v\"] - son_hafta.iloc[0][\"t_v\"] - son_hafta[\"d_inv\"].sum()\n",
    ")\n",
    "haftalik_yuzde = round(\n",
    "    (1 - (son_hafta.iloc[-7][\"t_v\"] / son_hafta.iloc[-1][\"t_v\"])) * 100, 2\n",
    ")\n",
    "\n",
    "son_ay = gunluk_ozet[-30:]\n",
    "son_ay.reset_index(drop=True, inplace=True)\n",
    "aylik_net = son_ay.iloc[-1][\"t_v\"] - son_ay.iloc[0][\"t_v\"] - son_ay[\"d_inv\"].sum()\n",
    "aylik_yuzde = round(\n",
    "    (1 - (son_ay.iloc[0][\"t_v\"] / (son_ay.iloc[-1][\"t_v\"] - son_ay[\"d_inv\"].sum())))\n",
    "    * 100,\n",
    "    2,\n",
    ")\n",
    "\n",
    "son_gun = hisse_gunluk.query(\"date == @today\").sort_values(\n",
    "    by=\"t_v\", ascending=True\n",
    ")\n",
    "son_gun.dropna(how=\"any\", inplace=True)\n",
    "data_list = [\n",
    "    {\"name\": ticker, \"value\": round(value, 1)}\n",
    "    for ticker, value in son_gun[[\"ticker\", \"t_v\"]].values\n",
    "]\n",
    "data_list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obb",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
